<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>阴阳师脚本</title>
    <url>/2019/12/11/%E9%98%B4%E9%98%B3%E5%B8%88%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<p>阴阳师是一款唯美空灵的和风写意,经典的半即时回合制RPG,卡牌收集、养成类游戏。最近看到win32库挺有意思的，就拿阴阳师练手了。</p>
<a id="more"></a> 

<p>1 账号获取</p>
<p>淘宝上可以以1元的价格买到200个网易账号，当然最好自己注册（淘宝买的账号卖家以txt文档格式给你，注意复制清除格式粘贴再使用效果更佳）。</p>
<p>2 获取阴阳师截图。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos;测试能否窗口截图&apos;&apos;&apos;</span><br><span class="line">#获取后台窗口的句柄，注意后台窗口不能最小化</span><br><span class="line">Onmyoji =Window_Find_Element()</span><br><span class="line">Onmyoji_screenshot=Onmyoji.screenshot()</span><br><span class="line">#假如截取图片失败，重新截取图片</span><br><span class="line">while Onmyoji_screenshot==1:</span><br><span class="line">    time.sleep(1)</span><br><span class="line">    Onmyoji_screenshot=Onmyoji.screenshot()</span><br></pre></td></tr></table></figure>

<p>注意登录和抽卡的页面该方法获取不到，我采用的是opencv库来解决这问题，缺点是不能放后台运行脚本。</p>
<p>3 获取目标元素位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&apos;&apos;&apos;查找元素，并返回元素位置&apos;&apos;&apos;</span><br><span class="line">class Window_Find_Element():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.hwnd=win32gui.FindWindow(0, u&apos;阴阳师-网易游戏&apos;)</span><br><span class="line">    #返回阴阳师窗口的灰度图</span><br><span class="line">    def screenshot(self):</span><br><span class="line">        #获取窗口的左上，右下的坐标</span><br><span class="line">        try:</span><br><span class="line">            self.l1, self.t1, self.r1, self.b1 = win32gui.GetWindowRect(self.hwnd)</span><br><span class="line">            self.l2, self.t2, self.r2, self.b2 = win32gui.GetClientRect(self.hwnd)</span><br><span class="line">            self._client_h = self.b2 - self.t2</span><br><span class="line">            self._client_w = self.r2 - self.l2</span><br><span class="line">            self._border_l = ((self.r1 - self.l1) - (self.r2 - self.l2)) // 2</span><br><span class="line">            self._border_t = ((self.b1 - self.t1) - (self.b2 - self.t2)) - self._border_l</span><br><span class="line">            self.hwindc = win32gui.GetWindowDC(self.hwnd)</span><br><span class="line">            self.srcdc = win32ui.CreateDCFromHandle(self.hwindc)</span><br><span class="line">            self.memdc = self.srcdc.CreateCompatibleDC()</span><br><span class="line">            self.bmp = win32ui.CreateBitmap()</span><br><span class="line">            self.bmp.CreateCompatibleBitmap(self.srcdc, self._client_w, self._client_h)</span><br><span class="line">            self.memdc.SelectObject(self.bmp)</span><br><span class="line">            self.memdc.BitBlt((0, 0), (self._client_w, self._client_h), self.srcdc,</span><br><span class="line">                         (self._border_l, self._border_t),</span><br><span class="line">                         win32con.SRCCOPY)</span><br><span class="line">            self.signedIntsArray = self.bmp.GetBitmapBits(True)</span><br><span class="line">            self.img = np.fromstring(self.signedIntsArray, dtype=&apos;uint8&apos;)</span><br><span class="line">                        try:</span><br><span class="line">                self.img.shape = (self._client_h, self._client_w, 4)</span><br><span class="line">                self.srcdc.DeleteDC()</span><br><span class="line">                self.memdc.DeleteDC()</span><br><span class="line">                win32gui.ReleaseDC(self.hwnd, self.hwindc)</span><br><span class="line">                win32gui.DeleteObject(self.bmp.GetHandle())</span><br><span class="line">                # cv2.imshow(&quot;image&quot;, cv2.cvtColor(self.img, cv2.COLOR_BGRA2GRAY))</span><br><span class="line">                # cv2.waitKey(0)</span><br><span class="line">            except:</span><br><span class="line">                print(&apos;获取窗口截图失败，正在重新获取窗口截图&apos;)</span><br><span class="line">                return 1</span><br><span class="line">        except:</span><br><span class="line">            print(&apos;请打开阴阳师&apos;)</span><br><span class="line">        self.img_gray = cv2.cvtColor(self.img, cv2.COLOR_RGB2GRAY)</span><br><span class="line">        #返回元素位置</span><br><span class="line">    def where_element(self,*args):</span><br><span class="line">        #获取元素大小</span><br><span class="line">        self.image_size = np.shape(*args)</span><br><span class="line">        #获取元素的高</span><br><span class="line">        self.th=self.image_size[0]</span><br><span class="line">        #获取元素的宽</span><br><span class="line">        self.tw=self.image_size[1]</span><br><span class="line">        #元素匹配 匹配算法为cv2.TM_CCOEFF_NORMED</span><br><span class="line">        self.result = cv2.matchTemplate(self.img_gray, *args, cv2.TM_CCOEFF_NORMED)</span><br><span class="line">        self.minVal, self.maxVal, self.minLoc, self.maxLoc = cv2.minMaxLoc(self.result)</span><br><span class="line">        if self.maxVal&lt;=0.5:</span><br><span class="line">            return 1</span><br><span class="line">        else:</span><br><span class="line">            self.element_left_top=self.maxLoc</span><br><span class="line">            self.element_right_down = (self.maxLoc[0] + self.tw*0.8, self.maxLoc[1] + self.th)</span><br><span class="line">            return self.element_left_top,self.element_right_down</span><br></pre></td></tr></table></figure>

<p>4 点击元素。</p>
<p>注意使用随机数防封号。代码中的图片为阴阳师登录页面的账号密码截图。图片如下：</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/%E9%98%B4%E9%98%B3%E5%B8%88%E8%84%9A%E6%9C%AC/youxiangzhuanghao.png" alt=""></p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/%E9%98%B4%E9%98%B3%E5%B8%88%E8%84%9A%E6%9C%AC/youxiangmima.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#鼠标后台点击</span><br><span class="line">    def mouse_click_bg(self, pos, pos_end=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        后台鼠标单击</span><br><span class="line">            :param pos: (x,y) 鼠标单击的坐标</span><br><span class="line">            :param pos_end=None: (x,y) 若pos_end不为空，则鼠标单击以pos为左上角坐标pos_end为右下角坐标的区域内的随机位置</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if pos_end == None:</span><br><span class="line">            win32gui.SendMessage(</span><br><span class="line">                self.hwnd, win32con.WM_MOUSEMOVE, 0, win32api.MAKELONG(pos[0], pos[1]))</span><br><span class="line">            win32gui.SendMessage(</span><br><span class="line">                self.hwnd, win32con.WM_LBUTTONDOWN, 0, win32api.MAKELONG(pos[0], pos[1]))</span><br><span class="line">            time.sleep(np.random.randint(20, 80) / 1000)</span><br><span class="line">            win32gui.SendMessage(</span><br><span class="line">                self.hwnd, win32con.WM_LBUTTONUP, 0, win32api.MAKELONG(pos[0], pos[1]))</span><br><span class="line">        else:</span><br><span class="line">            pos_rand = (np.random.randint(</span><br><span class="line">                pos[0], pos_end[0]), np.random.randint(pos[1], pos_end[1]))</span><br><span class="line">            win32gui.SendMessage(self.hwnd, win32con.WM_MOUSEMOVE,</span><br><span class="line">                                 0, win32api.MAKELONG(pos_rand[0], pos_rand[1]))</span><br><span class="line">            win32gui.SendMessage(self.hwnd, win32con.WM_LBUTTONDOWN, 0, win32api.MAKELONG(</span><br><span class="line">                pos_rand[0], pos_rand[1]))</span><br><span class="line">            time.sleep(np.random.randint(20, 80) / 1000)</span><br><span class="line">            win32gui.SendMessage(self.hwnd, win32con.WM_LBUTTONUP,</span><br><span class="line">                                 0, win32api.MAKELONG(pos_rand[0], pos_rand[1]))</span><br></pre></td></tr></table></figure>

<p>5 登录页面采用opencv库解决</p>
<p>当然抽卡也得用该方法解决</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class run_Onmyoji():</span><br><span class="line">    def __init__(self,username ,password):</span><br><span class="line">        self.username=username</span><br><span class="line">        self.password=password</span><br><span class="line">     #填写账号密码</span><br><span class="line">    def pass_word(self):</span><br><span class="line">        self.where_youxiangzhuanghao=r&apos;D:\1pc\img\youxiangzhuanghao.png&apos;</span><br><span class="line">        self.where_youxiangmima=r&apos;D:\1pc\img\youxiangmima.png&apos;</span><br><span class="line">        self.image_zhanghao = cv2.imread(self.where_youxiangzhuanghao,0)</span><br><span class="line">        self.image_mima =cv2.imread(self.where_youxiangmima,0)</span><br><span class="line">        # print(self.image_zhanghao)</span><br><span class="line">        # print(self.image_mima)</span><br><span class="line">        #屏幕截图</span><br><span class="line">        self.imscreenshot = pyautogui.screenshot()</span><br><span class="line">        self.imscreenshot = np.array(self.imscreenshot)</span><br><span class="line">        #获取屏幕灰白图像</span><br><span class="line">        self.imscreenshot = cv2.cvtColor(self.imscreenshot, cv2.COLOR_BGRA2GRAY)</span><br><span class="line">        &apos;&apos;&apos;找出账号填写位置&apos;&apos;&apos;</span><br><span class="line">        # 获取元素大小</span><br><span class="line">        #print(np.shape(self.image_zhanghao))</span><br><span class="line">        self.image_size = np.shape(self.image_zhanghao)</span><br><span class="line">        # 获取元素的高</span><br><span class="line">        self.th = self.image_size[0]</span><br><span class="line">        # 获取元素的宽</span><br><span class="line">        self.tw = self.image_size[1]</span><br><span class="line">        self.result = cv2.matchTemplate(self.imscreenshot, self.image_zhanghao, cv2.TM_CCOEFF_NORMED)</span><br><span class="line">        self.minVal, self.maxVal, self.minLoc, self.maxLoc = cv2.minMaxLoc(self.result)</span><br><span class="line">        if self.maxVal&lt;0.5:</span><br><span class="line">            print(&apos;不要盖住登录区&apos;)</span><br><span class="line">        else:</span><br><span class="line">            self.element_centre = (self.maxLoc[0] + self.tw/2, self.maxLoc[1] + self.th/2)</span><br><span class="line">        pyautogui.click(self.element_centre[0],self.element_centre[1])</span><br><span class="line">        self.username1=self.username.split(&apos;@&apos;)[0]</span><br><span class="line">        # print(self.username1)</span><br><span class="line">        pyautogui.typewrite(message=self.username1, interval=0.5)</span><br><span class="line">        pyautogui.click(self.element_centre[0], self.element_centre[1]+30)</span><br><span class="line">     #填写用户密码</span><br><span class="line">        &apos;&apos;&apos;找出用户密码填写位置&apos;&apos;&apos;</span><br><span class="line">        self.image_size_2 = np.shape(self.image_mima)</span><br><span class="line">        # 获取元素的高</span><br><span class="line">        self.th_2 = self.image_size_2[0]</span><br><span class="line">        # 获取元素的宽</span><br><span class="line">        self.tw_2 = self.image_size_2[1]</span><br><span class="line">        self.result_2 = cv2.matchTemplate(self.imscreenshot, self.image_mima, cv2.TM_CCOEFF_NORMED)</span><br><span class="line">        self.minVal_2, self.maxVal_2, self.minLoc_2, self.maxLoc_2 = cv2.minMaxLoc(self.result_2)</span><br><span class="line">        if self.maxVal_2 &lt; 0.5:</span><br><span class="line">            print(&apos;不要盖住登录区&apos;)</span><br><span class="line">        else:</span><br><span class="line">            self.element_centre_2 = (self.maxLoc_2[0] + self.tw_2 / 2, self.maxLoc_2[1] + self.th_2 / 2)</span><br><span class="line">        pyautogui.click(self.element_centre_2[0], self.element_centre_2[1])</span><br><span class="line">        pyautogui.typewrite(message=self.password, interval=0.5)</span><br></pre></td></tr></table></figure>

<p>6 图像匹配位置点击</p>
<p>path 是指定元素的截图位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定位置点击</span><br><span class="line">    def dianji(self,path,xiu=0):</span><br><span class="line">        self.where_dianji = path</span><br><span class="line">        self.image_dianji = cv2.imread(self.where_dianji, 0)</span><br><span class="line">        # 屏幕截图</span><br><span class="line">        self.imscreenshot = pyautogui.screenshot()</span><br><span class="line">        self.imscreenshot = np.array(self.imscreenshot)</span><br><span class="line">        # 获取屏幕灰白图像</span><br><span class="line">        self.imscreenshot = cv2.cvtColor(self.imscreenshot, cv2.COLOR_BGRA2GRAY)</span><br><span class="line">        # 获取元素大小</span><br><span class="line">        # print(np.shape(self.image_zhanghao))</span><br><span class="line">        self.image_size_3 = np.shape(self.image_dianji)</span><br><span class="line">        self.result_3 = cv2.matchTemplate(self.imscreenshot, self.image_dianji, cv2.TM_CCOEFF_NORMED)</span><br><span class="line">        self.minVal_3, self.maxVal_3, self.minLoc_3, self.maxLoc_3 = cv2.minMaxLoc(self.result_3)</span><br><span class="line">        if self.maxVal_3&lt;0.5:</span><br><span class="line">            print(&apos;不要盖住阴阳师&apos;)</span><br><span class="line">        else:</span><br><span class="line">            self.element_centre_3 = (self.maxLoc_3[0] + self.image_size_3[0]/2, self.maxLoc_3[1] + self.image_size_3[1]/2)</span><br><span class="line">        if xiu==0:</span><br><span class="line">            pyautogui.click(self.element_centre_3[0], self.element_centre_3[1])</span><br><span class="line">        else:</span><br><span class="line">            pyautogui.click(self.element_centre_3[0], self.element_centre_3[1]-30)</span><br></pre></td></tr></table></figure>

<p>7 后台指定目标位置点击</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#鼠标后台点击</span><br><span class="line">    def mouse_click_bg(self, pos, pos_end=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        后台鼠标单击</span><br><span class="line">            :param pos: (x,y) 鼠标单击的坐标</span><br><span class="line">            :param pos_end=None: (x,y) 若pos_end不为空，则鼠标单击以pos为左上角坐标pos_end为右下角坐标的区域内的随机位置</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if pos_end == None:</span><br><span class="line">            win32gui.SendMessage(</span><br><span class="line">                self.hwnd, win32con.WM_MOUSEMOVE, 0, win32api.MAKELONG(pos[0], pos[1]))</span><br><span class="line">            win32gui.SendMessage(</span><br><span class="line">                self.hwnd, win32con.WM_LBUTTONDOWN, 0, win32api.MAKELONG(pos[0], pos[1]))</span><br><span class="line">            time.sleep(np.random.randint(20, 80) / 1000)</span><br><span class="line">            win32gui.SendMessage(</span><br><span class="line">                self.hwnd, win32con.WM_LBUTTONUP, 0, win32api.MAKELONG(pos[0], pos[1]))</span><br><span class="line">        else:</span><br><span class="line">            pos_rand = (np.random.randint(</span><br><span class="line">                pos[0], pos_end[0]), np.random.randint(pos[1], pos_end[1]))</span><br><span class="line">            win32gui.SendMessage(self.hwnd, win32con.WM_MOUSEMOVE,</span><br><span class="line">                                 0, win32api.MAKELONG(pos_rand[0], pos_rand[1]))</span><br><span class="line">            win32gui.SendMessage(self.hwnd, win32con.WM_LBUTTONDOWN, 0, win32api.MAKELONG(</span><br><span class="line">                pos_rand[0], pos_rand[1]))</span><br><span class="line">            time.sleep(np.random.randint(20, 80) / 1000)</span><br><span class="line">            win32gui.SendMessage(self.hwnd, win32con.WM_LBUTTONUP,</span><br><span class="line">                                 0, win32api.MAKELONG(pos_rand[0], pos_rand[1]))</span><br></pre></td></tr></table></figure>

<p>8，休眠</p>
<p>有时点击后系统需要反应时间，所以需休眠几秒，这个实际情况调整。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with open(r&apos;D:\1pc\img\2019_12_6&apos;) as f:</span><br><span class="line">    line = f.readline()</span><br><span class="line">    print(line)</span><br><span class="line">    #读取单个账号密码</span><br><span class="line">    count=1</span><br><span class="line">    while line:</span><br><span class="line">        print(line)</span><br><span class="line">        username=line.split(&apos;-&apos;)[0]</span><br><span class="line">        password=line[-7:]</span><br><span class="line">        #第一步登录账号</span><br><span class="line">        # print(username)</span><br><span class="line">        # print(password)</span><br><span class="line">        run_start=run_Onmyoji(username,password)</span><br><span class="line">        run_start.pass_word()</span><br><span class="line">        #第二步，选择安卓账号,关闭公告</span><br><span class="line">        time.sleep(3)</span><br><span class="line">        run_start.dianji(r&apos;D:\1pc\img\anzuo.png&apos;)</span><br><span class="line">        time.sleep(3)</span><br><span class="line">        run_start.dianji(r&apos;D:\1pc\img\gonggao.png&apos;)</span><br><span class="line">        #点击进入游戏</span><br><span class="line">        time.sleep(3)</span><br><span class="line">        run_start.dianji(r&apos;D:\1pc\img\jingruyouxi.png&apos;)</span><br><span class="line">        #随机创建角色名</span><br><span class="line">        time.sleep(3)</span><br><span class="line">        run_start.dianji(r&apos;D:\1pc\img\chuangjianmingzi.png&apos;)</span><br><span class="line">        #确定角色名</span><br><span class="line">        time.sleep(1)</span><br><span class="line">        run_start.dianji(r&apos;D:\1pc\img\chuangjianjueshe.png&apos;,1)</span><br><span class="line">        #开始过新手教程</span><br><span class="line">        #点击16次（786，555）完成小白对话</span><br><span class="line">        for i in range(16):</span><br><span class="line">            Onmyoji.mouse_click_bg((786, 555))</span><br><span class="line">            sleep1(1)</span><br><span class="line">		#点击1次（784，245）开始犬神对话</span><br><span class="line">        Onmyoji.mouse_click_bg((784, 245))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        #点击（778，510），跳过对话。如果不想跳过对话，休眠几秒。</span><br><span class="line">        Onmyoji.mouse_click_bg((778, 510))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        #点击（778，510），跳过犬神插画。</span><br><span class="line">        Onmyoji.mouse_click_bg((778, 510))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        #点击点击（779，507）跳过神乐对话。</span><br><span class="line">        Onmyoji.mouse_click_bg((779, 507))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        #点击（881，623），晴明普攻</span><br><span class="line">        Onmyoji.mouse_click_bg((881, 623))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        #点击点击2次（704，207）选中普攻目标，休眠1秒再点击一次。跳过动画，</span><br><span class="line">        Onmyoji.mouse_click_bg((704, 207))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        Onmyoji.mouse_click_bg((704, 207))</span><br><span class="line">        sleep1(2)</span><br><span class="line">		#点击点击（993，625），晴明使用技能</span><br><span class="line">        Onmyoji.mouse_click_bg((993, 625))</span><br><span class="line">        sleep1(2)</span><br><span class="line">        #点击（436，353）选中技能目标。休眠1秒等待，再点一次</span><br><span class="line">        Onmyoji.mouse_click_bg((436, 353))</span><br><span class="line">        sleep1(2)</span><br><span class="line">		#点击（700，200），选择普攻目标。</span><br><span class="line">        Onmyoji.mouse_click_bg((700, 200))</span><br><span class="line">        sleep1(2)</span><br><span class="line">        #点击（704，200）普攻，休眠2秒， </span><br><span class="line">        Onmyoji.mouse_click_bg((704, 200))</span><br><span class="line">        sleep1(2)</span><br><span class="line">        #再普攻一次</span><br><span class="line">        Onmyoji.mouse_click_bg((704, 200))</span><br><span class="line">        sleep1(5)</span><br><span class="line">		#再点击（570，500）结算</span><br><span class="line">        Onmyoji.mouse_click_bg((570, 500))</span><br><span class="line">        sleep1(5)</span><br><span class="line">        #点击（390，300），开始对话</span><br><span class="line">        Onmyoji.mouse_click_bg((390, 300))</span><br><span class="line">        sleep1(2)</span><br><span class="line">        点击（783，508）跳过</span><br><span class="line">        Onmyoji.mouse_click_bg((783, 508))</span><br><span class="line">        sleep1(40)</span><br><span class="line">        Onmyoji.mouse_click_bg((436, 217))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        Onmyoji.mouse_click_bg((826, 404))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        Onmyoji.mouse_click_bg((778, 507))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        Onmyoji.mouse_click_bg((389, 303))</span><br><span class="line">        sleep1(1)</span><br><span class="line">        Onmyoji.mouse_click_bg((1089, 208))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((785, 506))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((255, 308))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((779, 512))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((897, 208))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((780, 508))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((81, 207))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((780, 509))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((786, 555))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        Onmyoji.mouse_click_bg((786, 555))</span><br><span class="line">        sleep1(10)</span><br><span class="line">        #召唤</span><br><span class="line">        Onmyoji.mouse_click_bg((988, 219))</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>python常用函数</title>
    <url>/2019/12/03/python%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>Python 是一个高层次的结合了解释性、编译性、互动性和面向对象的脚本语言。</p>
<p>Python 的设计具有很强的可读性，相比其他语言经常使用英文关键字，其他语言的一些标点符号，它具有比其他语言更有特色语法结构。</p>
<ul>
<li><strong>Python 是一种解释型语言：</strong> 这意味着开发过程中没有了编译这个环节。类似于PHP和Perl语言。</li>
<li><strong>Python 是交互式语言：</strong> 这意味着，您可以在一个 Python 提示符 <strong>&gt;&gt;&gt;</strong> 后直接执行代码。</li>
<li><strong>Python 是面向对象语言:</strong> 这意味着Python支持面向对象的风格或代码封装在对象的编程技术。</li>
</ul>
<a id="more"></a>

<h1 id="python常用函数"><a href="#python常用函数" class="headerlink" title="python常用函数"></a>python常用函数</h1><h2 id="1-map-函数"><a href="#1-map-函数" class="headerlink" title="1.map()函数"></a>1.map()函数</h2><p><strong>map()</strong>是 Python 内置的高阶函数，它接收一个<strong>函数 f</strong> 和一个<strong>list</strong>，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 并返回。</p>
<p>例如，对于list [1, 2, 3, 4, 5, 6, 7, 8, 9]</p>
<p>如果希望把list的每个元素都作平方，就可以用map()函数：</p>
<p>只需要传入函数f(x)=x*x，就可以利用map()函数完成这个计算：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">    return x*x</span><br><span class="line">print (map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9]))</span><br></pre></td></tr></table></figure>

<p><strong>输出结果：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1, 4, 9, 10, 25, 36, 49, 64, 81]</span><br></pre></td></tr></table></figure>

<h2 id="2-匿名函数lambda"><a href="#2-匿名函数lambda" class="headerlink" title="2.匿名函数lambda"></a>2.匿名函数lambda</h2><p>高阶函数可以接收函数做参数，有些时候，我们不需要显式地定义函数，直接传入匿名函数更方便。</p>
<p>在Python中，对匿名函数提供了有限支持。还是以map()函数为例，计算 f(x)=x2 时，除了定义一个f(x)的函数外，还可以直接传入匿名函数：</p>
<p>&gt;&gt;&gt; map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])</p>
<p>[1, 4, 9, 16, 25, 36, 49, 64, 81]</p>
<h2 id="3-自定义排序函数sorted"><a href="#3-自定义排序函数sorted" class="headerlink" title="3.自定义排序函数sorted()"></a>3.自定义排序函数sorted()</h2><p>Python内置的 <strong>sorted()</strong>函数可对list进行排序：</p>
<p>&gt;&gt;&gt;sorted([36, 5, 12, 9, 21])<br> [5, 9, 12, 21, 36]</p>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>pyautogui使用</title>
    <url>/2019/12/02/pyautogui%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>PyAutoGUI是一个纯Python的GUI自动化工具,其目的是可以用程序自动控制鼠标和键盘<br>操作多平台支持（Windows，OS X，Linux）。</p>
<a id="more"></a>

<h1 id="1-pyautogui安装"><a href="#1-pyautogui安装" class="headerlink" title="1.pyautogui安装"></a>1.pyautogui安装</h1><p>可以用pip install pyautogui安装</p>
<h1 id="2-pyautogui使用"><a href="#2-pyautogui使用" class="headerlink" title="2.pyautogui使用"></a>2.pyautogui使用</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pyautogui</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;获取当前屏幕分辨率&apos;&apos;&apos;</span><br><span class="line"># screenWidth, screenHeight = pyautogui.size()</span><br><span class="line"># print(screenHeight,screenWidth)</span><br><span class="line">&apos;&apos;&apos;获取当前鼠标位置&apos;&apos;&apos;</span><br><span class="line"># currentMouseX, currentMouseY = pyautogui.position()</span><br><span class="line"># print(currentMouseX,currentMouseY)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;2秒钟鼠标移动坐标为100,100位置  绝对移动&apos;&apos;&apos;</span><br><span class="line"># pyautogui.moveTo(100, 100,2)</span><br><span class="line"># pyautogui.moveTo(x=100, y=100,duration=2, tween=pyautogui.linear)</span><br><span class="line">&apos;&apos;&apos;鼠标相对移动 ,</span><br><span class="line">xOffset:值为正向右移动。值为负向左移动。</span><br><span class="line">yOffset：值为正向下移动；值为负向上移动</span><br><span class="line">pyautogui.moveRel(None, 10)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># pyautogui.moveRel(xOffset=100, yOffset=100,duration=0.0, tween=pyautogui.linear)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;用缓动/渐变函数让鼠标2秒后移动到(500,500)位置&apos;&apos;&apos;</span><br><span class="line"># #  use tweening/easing function to move mouse over 2 seconds.</span><br><span class="line"># pyautogui.moveTo(x=500, y=500, duration=2, tween=pyautogui.easeInOutQuad)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">鼠标左击一次</span><br><span class="line">pyautogui.click()</span><br><span class="line">x</span><br><span class="line">y</span><br><span class="line">clicks 点击次数</span><br><span class="line">interval点击之间的间隔</span><br><span class="line">button &apos;left&apos;, &apos;middle&apos;, &apos;right&apos; 对应鼠标 左 中 右或者取值(1, 2, or 3)</span><br><span class="line">tween 渐变函数</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># pyautogui.click(x=None, y=None, clicks=1, interval=0.0, button=&apos;left&apos;, duration=0.0, tween=pyautogui.linear)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;鼠标当前位置0间隔双击&apos;&apos;&apos;</span><br><span class="line"># #pyautogui.doubleClick()</span><br><span class="line"># pyautogui.doubleClick(x=None, y=None, interval=0.0, button=&apos;left&apos;, duration=0.0, tween=pyautogui.linear)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;鼠标当前位置3击&apos;&apos;&apos;</span><br><span class="line"># #pyautogui.tripleClick()</span><br><span class="line"># pyautogui.tripleClick(x=None, y=None, interval=0.0, button=&apos;left&apos;, duration=0.0, tween=pyautogui.linear)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;鼠标右击&apos;&apos;&apos;</span><br><span class="line"># pyautogui.rightClick()</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;鼠标中击&apos;&apos;&apos;</span><br><span class="line"># pyautogui.middleClick()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;鼠标拖拽操作&apos;&apos;&apos;</span><br><span class="line"># #鼠标拖拽</span><br><span class="line"># pyautogui.dragTo(x=427, y=535, duration=3,button=&apos;left&apos;)</span><br><span class="line"># #鼠标相对拖拽</span><br><span class="line"># pyautogui.dragRel(xOffset=100,yOffset=100,duration=3,button=&apos;left&apos;,mouseDownUp=False)</span><br><span class="line"># #鼠标移动到x=1796, y=778位置按下</span><br><span class="line"># pyautogui.mouseDown(x=1796, y=778, button=&apos;left&apos;)</span><br><span class="line"># #鼠标移动到x=2745, y=778位置松开（与mouseDown组合使用选中）</span><br><span class="line"># pyautogui.mouseUp(x=2745, y=778, button=&apos;left&apos;,duration=5)</span><br><span class="line"># #鼠标当前位置滚轮滚动</span><br><span class="line"># pyautogui.scroll()</span><br><span class="line"># #鼠标水平滚动（Linux）</span><br><span class="line"># pyautogui.hscroll()</span><br><span class="line"># #鼠标左右滚动（Linux）</span><br><span class="line"># pyautogui.vscroll()</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;键盘操作&apos;&apos;&apos;</span><br><span class="line"># #模拟输入信息</span><br><span class="line"># pyautogui.typewrite(message=&apos;Hello world!&apos;,interval=0.5)</span><br><span class="line"># #点击ESC</span><br><span class="line"># pyautogui.press(&apos;esc&apos;)</span><br><span class="line"># # 按住shift键</span><br><span class="line"># pyautogui.keyDown(&apos;shift&apos;)</span><br><span class="line"># # 放开shift键</span><br><span class="line"># pyautogui.keyUp(&apos;shift&apos;)</span><br><span class="line"># # 模拟组合热键</span><br><span class="line"># pyautogui.hotkey(&apos;ctrl&apos;, &apos;c&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;截屏并保存&apos;&apos;&apos;</span><br><span class="line"># im1 = pyautogui.screenshot()</span><br><span class="line"># im1.save(&apos;my_screenshot.png&apos;)</span><br><span class="line">#</span><br><span class="line"># im2 = pyautogui.screenshot(&apos;my_screenshot2.png&apos;)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;屏幕查找图片位置并获取中间点&apos;&apos;&apos;</span><br><span class="line"># #在当前屏幕中查找指定图片(图片需要由系统截图功能截取的图)</span><br><span class="line"># coords = pyautogui.locateOnScreen(&apos;folder.png&apos;)</span><br><span class="line"># #获取定位到的图中间点坐标</span><br><span class="line"># x,y=pyautogui.center(coords)</span><br><span class="line"># #右击该坐标点</span><br><span class="line"># pyautogui.rightClick(x,y)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;安全措施&apos;&apos;&apos;</span><br><span class="line"># #保护措施，避免失控</span><br><span class="line"># pyautogui.FAILSAFE = True</span><br><span class="line"># #为所有的PyAutoGUI函数增加延迟。默认延迟时间是0.1秒。</span><br><span class="line"># pyautogui.PAUSE = 0.5</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN模型</title>
    <url>/2019/12/01/GAN%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p> 生成对抗网络（Generative Adversarial Network）由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent space）中随机采样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。 ——<a href="https://zh.wikipedia.org/wiki/生成对抗网络" target="_blank" rel="noopener">维基百科</a> </p>
<a id="more"></a>

<h1 id="1-GAN-模型结构"><a href="#1-GAN-模型结构" class="headerlink" title="1. GAN 模型结构"></a>1. GAN 模型结构</h1><p> GAN基本结构包括两个网络结构——生成模型Generator和判别模型Discriminator。G网络尽可能生成满足正样本分布的假样本，而D网络则尽可能辨别出真假样本，在这个博弈过程中2种网络的性能都越来越好。GAN模型结构如下： </p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/GAN%E6%A8%A1%E5%9E%8B/gan1.png" alt=""></p>
<h1 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2. 代码实现"></a>2. 代码实现</h1><p>数据集：mnist数据集的0数字图片。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#定义输入数据格式</span><br><span class="line">def get_inputs(real_size, noise_size):</span><br><span class="line">    real_img = tf.placeholder(tf.float32, [None, real_size], name=&quot;real_img&quot;)</span><br><span class="line">    noise_img = tf.placeholder(tf.float32, [None, noise_size], name=&quot;noise_img&quot;)</span><br><span class="line">    return real_img, noise_img</span><br><span class="line"></span><br><span class="line">#将一张随机像素的图片经过一个全连接层后经过一个Leaky ReLU处理，</span><br><span class="line">#之后为了避免过拟合dropout后再经过一个全连接层进行tanh激活后，生成一张“假图片”</span><br><span class="line">def get_generator(noise_img, n_units, out_dim, reuse=False, alpha=0.01):</span><br><span class="line">    with tf.variable_scope(&quot;generator&quot;, reuse=reuse):</span><br><span class="line">        hidden1 = tf.layers.dense(noise_img, n_units)  # 全连接层</span><br><span class="line">        hidden1 = tf.maximum(alpha * hidden1, hidden1)</span><br><span class="line">        hidden1 = tf.layers.dropout(hidden1, rate=0.2)</span><br><span class="line">        logits = tf.layers.dense(hidden1, out_dim)</span><br><span class="line">        outputs = tf.tanh(logits)</span><br><span class="line">        return logits, outputs</span><br><span class="line">#将待判定的图片经过全连接层--&gt;Leaky ReLU--&gt;全连接层--&gt;sigmoid激活函数处理后，得到0或1的结果。</span><br><span class="line">def get_discriminator(img, n_units, reuse=False, alpha=0.01):</span><br><span class="line">    with tf.variable_scope(&quot;discriminator&quot;, reuse=reuse):</span><br><span class="line">        hidden1 = tf.layers.dense(img, n_units)</span><br><span class="line">        hidden1 = tf.maximum(alpha * hidden1, hidden1)</span><br><span class="line">        logits = tf.layers.dense(hidden1, 1)</span><br><span class="line">        outputs = tf.sigmoid(logits)</span><br><span class="line">        return logits, outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#在实现时，我们可以首先把MNIST数据中的标签为0的图像提取出来，存到列表中，总共取了5000张图片。</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;mnist/&quot;, one_hot=True) #自己建一个文件夹mnist</span><br><span class="line">samples=[]</span><br><span class="line">i = j = 0</span><br><span class="line">while i&lt;5000:</span><br><span class="line">    if np.argmax(mnist.train.labels[j])== 0:</span><br><span class="line">        samples.append(mnist.train.images[j])</span><br><span class="line">        i += 1</span><br><span class="line">    j += 1</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">img_size:输入数据大小</span><br><span class="line">noise_size:生成器输入数据大小</span><br><span class="line">g_units d_units 超参数</span><br><span class="line">learning_rate 学习率</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">img_size = 784</span><br><span class="line">noise_size = 100</span><br><span class="line">g_units = 128</span><br><span class="line">d_units = 128</span><br><span class="line">alpha = 0.01</span><br><span class="line">learning_rate = 0.001</span><br><span class="line">smooth = 0.1</span><br><span class="line"></span><br><span class="line">tf.reset_default_graph() #清除默认图形堆栈并重置全局默认图形。</span><br><span class="line">real_img, noise_img = get_inputs(img_size, noise_size)</span><br><span class="line">g_logits, g_outputs = get_generator(noise_img, g_units, img_size)</span><br><span class="line"></span><br><span class="line">d_logits_real, d_outputs_real = get_discriminator(real_img, d_units)</span><br><span class="line">d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, d_units, reuse=True)</span><br><span class="line"></span><br><span class="line">d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</span><br><span class="line">    logits=d_logits_real, labels=tf.ones_like(d_logits_real)</span><br><span class="line">) * (1 - smooth))</span><br><span class="line">d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</span><br><span class="line">    logits=d_logits_fake, labels=tf.zeros_like(d_logits_fake)</span><br><span class="line">))</span><br><span class="line">d_loss = tf.add(d_loss_real, d_loss_fake)</span><br><span class="line">g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</span><br><span class="line">    logits=d_logits_fake, labels=tf.ones_like(d_logits_fake)</span><br><span class="line">) * (1 - smooth))</span><br><span class="line"></span><br><span class="line">train_vars = tf.trainable_variables()</span><br><span class="line">g_vars = [var for var in train_vars if var.name.startswith(&quot;generator&quot;)]</span><br><span class="line">d_vars = [var for var in train_vars if var.name.startswith(&quot;discriminator&quot;)]</span><br><span class="line"></span><br><span class="line">d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)</span><br><span class="line">g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">epochs = 5000</span><br><span class="line">n_sample = 10</span><br><span class="line">losses = []</span><br><span class="line"></span><br><span class="line">print(len(samples))</span><br><span class="line">size = samples[0].size</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    for e in range(epochs):</span><br><span class="line">        batch_images = samples[e] * 2 -1</span><br><span class="line">        batch_noise = np.random.uniform(-1, 1, size=noise_size)</span><br><span class="line"></span><br><span class="line">        _ = sess.run(d_train_opt, feed_dict=&#123;real_img:[batch_images], noise_img:[batch_noise]&#125;)</span><br><span class="line">        _ = sess.run(g_train_opt, feed_dict=&#123;noise_img:[batch_noise]&#125;)</span><br><span class="line"></span><br><span class="line">    sample_noise = np.random.uniform(-1, 1, size=noise_size)</span><br><span class="line">    g_logit, g_output = sess.run(get_generator(noise_img, g_units, img_size,</span><br><span class="line">                                         reuse=True), feed_dict=&#123;</span><br><span class="line">        noise_img:[sample_noise]</span><br><span class="line">    &#125;)</span><br><span class="line">    print(g_logit.size)</span><br><span class="line">    g_output = (g_output+1)/2</span><br><span class="line">    plt.imshow(g_output.reshape([28, 28]), cmap=&apos;Greys_r&apos;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>训练5000次后生成图片的结果：</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/GAN%E6%A8%A1%E5%9E%8B/GAN2.png" alt=""></p>
<p> 可以看出，在经过了5000次的迭代后，g网络生成的图片已经可以大致呈现出一个0的形状。 </p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>VAE模型</title>
    <url>/2019/11/30/vae/</url>
    <content><![CDATA[<p>VAE 全名叫 变分自编码器，是从之前的 auto-encoder 演变过来的，auto-encoder 也就是自编码器，自编码器，顾名思义，就是可以自己对自己进行编码，重构。所以 AE 模型一般都由两部分的网络构成，一部分称为 encoder, 从一个高维的输入映射到一个低维的隐变量上，另外一部分称为 decoder, 从低维的隐变量再映射回高维的输入。</p>
<a id="more"></a>

<p>目前具有潜力的图像生成模型主要两大类，分别是变分自编码和生成式对抗网络（Gan）。Gan模型联合一个合成图像生成器和一个图像分类的鉴别器生成图像，Gan模型生成的图像为了骗过鉴别器而自由发挥会造成图像的失真。VAE模型使用变分推理联合学习图像和潜在代码之间的编码器和解码器映射，虽然生成的图像没有Gan模型生成的图像清晰，但是生成的图像准确。在这里用手写数据集做了验证。  </p>
<h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy.stats import norm</span><br><span class="line"></span><br><span class="line">from keras.layers import Input, Dense, Lambda</span><br><span class="line">from keras.models import Model</span><br><span class="line">from keras.regularizers import l2</span><br><span class="line">from keras import backend as K</span><br><span class="line">from keras import objectives</span><br><span class="line">from keras.datasets import mnist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = 100</span><br><span class="line">n = 784</span><br><span class="line">m = 2</span><br><span class="line">hidden_dim = 256</span><br><span class="line">epochs = 50</span><br><span class="line">epsilon_std = 1.0</span><br><span class="line">use_loss = &apos;xent&apos; # 这里提供2种激活函数[&apos;xent&apos;,&apos;mse&apos;],具体看vae_loss函数</span><br><span class="line"></span><br><span class="line">decay = 1e-4 # weight decay, a.k. l2 regularization</span><br><span class="line">use_bias = True</span><br><span class="line"></span><br><span class="line">## Encoder</span><br><span class="line">x = Input(batch_shape=(batch_size, n))</span><br><span class="line">h_encoded = Dense(hidden_dim, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=use_bias, activation=&apos;tanh&apos;)(x)</span><br><span class="line">z_mean = Dense(m, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=use_bias)(h_encoded)</span><br><span class="line">z_log_var = Dense(m, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=use_bias)(h_encoded)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Sampler</span><br><span class="line">def sampling(args):</span><br><span class="line">    z_mean, z_log_var = args</span><br><span class="line">    epsilon = K.random_normal_variable(shape=(batch_size, m), mean=0.,</span><br><span class="line">                                       scale=epsilon_std)</span><br><span class="line">    return z_mean + K.exp(z_log_var / 2) * epsilon</span><br><span class="line"></span><br><span class="line">z = Lambda(sampling, output_shape=(m,))([z_mean, z_log_var])</span><br><span class="line"></span><br><span class="line"># we instantiate these layers separately so as to reuse them later</span><br><span class="line">decoder_h = Dense(hidden_dim, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=use_bias, activation=&apos;tanh&apos;)</span><br><span class="line">decoder_mean = Dense(n, kernel_regularizer=l2(decay), bias_regularizer=l2(decay), use_bias=use_bias, activation=&apos;sigmoid&apos;)</span><br><span class="line"></span><br><span class="line">## Decoder</span><br><span class="line">h_decoded = decoder_h(z)</span><br><span class="line">x_hat = decoder_mean(h_decoded)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## loss</span><br><span class="line">def vae_loss(x, x_hat):</span><br><span class="line">    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)</span><br><span class="line">    xent_loss = n * objectives.binary_crossentropy(x, x_hat)</span><br><span class="line">    mse_loss = n * objectives.mse(x, x_hat)</span><br><span class="line">    if use_loss == &apos;xent&apos;:</span><br><span class="line">        return xent_loss + kl_loss</span><br><span class="line">    elif use_loss == &apos;mse&apos;:</span><br><span class="line">        return mse_loss + kl_loss</span><br><span class="line">    else:</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">vae = Model(x, x_hat)</span><br><span class="line">vae.compile(optimizer=&apos;rmsprop&apos;, loss=vae_loss)</span><br><span class="line"></span><br><span class="line"># train the VAE on MNIST digits</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">x_train = x_train.astype(&apos;float32&apos;) / 255.</span><br><span class="line">x_test = x_test.astype(&apos;float32&apos;) / 255.</span><br><span class="line">x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))</span><br><span class="line">x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))</span><br><span class="line"></span><br><span class="line"># build a digit generator that can sample from the learned distribution</span><br><span class="line">decoder_input = Input(shape=(m,))</span><br><span class="line">_h_decoded = decoder_h(decoder_input)</span><br><span class="line">_x_hat = decoder_mean(_h_decoded)</span><br><span class="line">generator = Model(decoder_input, _x_hat)</span><br><span class="line"></span><br><span class="line">import random</span><br><span class="line">n_size=10</span><br><span class="line">data_show1=[]</span><br><span class="line">for i in range(10):</span><br><span class="line">    vae.fit(x_train, x_train,</span><br><span class="line">        shuffle=True,</span><br><span class="line">        epochs=epochs,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        validation_data=(x_test, x_test))</span><br><span class="line"></span><br><span class="line">    z_sample=[random.random() for _ in range(2*n_size)]</span><br><span class="line">    z_sample=np.reshape(a,[n_size,2])</span><br><span class="line">    x_decoded = generator.predict(z_sample)</span><br><span class="line">    for j in range(n_size):</span><br><span class="line">        if j == 0:</span><br><span class="line">            data_show=x_decoded[j].reshape(28,28)</span><br><span class="line">        else:</span><br><span class="line">            data_show=np.concatenate([data_show,x_decoded[j].reshape(28,28)],axis=1)</span><br><span class="line">    data_show1.append(data_show)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">data_show2=np.reshape(data_show1,[-1,280])</span><br><span class="line">plt.imshow(data_show2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>每训练50次输出生成的10个数字，共训练了500次生成100个数字如下图：</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/vae%E6%A8%A1%E5%9E%8B/vae.png" alt=""></p>
<p>当然可以挑选手写数据集中每个数字10张清晰的图片，共100张图片训练，效果会更明显，如下图：</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/vae%E6%A8%A1%E5%9E%8B/vae100.png" alt=""></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>som算法</title>
    <url>/2019/11/28/som%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p> 生物学研究表明，在人脑的感觉通道上，神经元的组织原理是有序排列的。当外界的特定时空信息输入时，大脑皮层的特定区域兴奋，而且类似的外界信息在对应的区域是连续映像的。生物视网膜中有许多特定的细胞对特定的图形比较敏感，当视网膜中有若干个接收单元同时受特定模式刺激时，就使大脑皮层中的特定神经元开始兴奋，输入模式接近，与之对应的兴奋神经元也接近；在听觉通道上，神经元在结构排列上与频率的关系十分密切，对于某个频率，特定的神经元具有最大的响应，位置相邻的神经元具有相近的频率特征，而远离的神经元具有的频率特征差别也较大。大脑皮层中神经元的这种响应特点不是先天安排好的，而是通过后天的学习自组织形成的 。 在生物神经系统中，存在着一种侧抑制现象，即一个神经细胞兴奋以后，会对周围其他神经细胞产生抑制作用。这种抑制作用会使神经细胞之间出现竞争，其结果是某些获胜，而另一些则失败。表现形式是获胜神经细胞兴奋，失败神经细胞抑制。自组织（竞争型）神经网络就是模拟上述生物神经系统功能的人工神经网络 。</p>
<a id="more"></a>

<h1 id="1-SOM简介"><a href="#1-SOM简介" class="headerlink" title="1.SOM简介"></a>1.SOM简介</h1><p> SOM 即自组织映射，是一种用于特征检测的无监督学习神经网络。它模拟人脑中处于不同区域的神经细胞分工不同的特点，即不同区域具有不同的响应特征，而且这一过程是自动完成的。SOM 用于生成训练样本的低维空间，可以将高维数据间复杂的非线性统计关系转化为简单的几何关系，且以低维的方式展现，因此通常在降维问题中会使用它。 </p>
<p>SOM 与其它人工神经网络不同，因为它们使用的是竞争性学习而不是错误相关的学习，后者涉及到反向传播和梯度下降。在竞争性学习中，各个节点会相互竞争响应输入数据子集的权利。训练数据通常没有标签，映射会学习根据相似度来区分各个特征。 </p>
<h2 id="2-SOM结构"><a href="#2-SOM结构" class="headerlink" title="2 .SOM结构"></a>2 .SOM结构</h2><p>SOM结构如下图,它由输入层和竞争层(输出层)组成。输入层神经元数为 n,竞争层由m个神经元组成的一维或者二维的平面阵列，网络是全连接的即每个输入节点都与所有的输出节点相连接。</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/som%E7%AE%97%E6%B3%95/som.png" alt=""></p>
<h1 id="3-SOM学习过程"><a href="#3-SOM学习过程" class="headerlink" title="3 .SOM学习过程"></a>3 .SOM学习过程</h1><p>SOM是无监督学习, 不需要成对的数据. SOM的学习过程可以总结成一句话, 就是调整输出层权重W以便于让相邻的节点具有相似的权重. 具体来说分为一下的步骤:</p>
<ol>
<li>每个节点的权重都被随机初始化。</li>
<li>从训练数据集中随机选择向量x并将其输入到网格。</li>
<li>检查每个输出层节点以计算哪一个的权重最像输入向量。获胜节点通常被称为最佳匹配单元（BMU）或激活单元。</li>
<li>现在计算BMU邻域节点, 以BMU为圆心, 处于半径R内的节点都被称为邻域。R是一个开始较大的值，通常设置为输出层的“半径”，但会随着训练逐步减小。在该半径内发现的任何节点都被视为在BMU的邻域内。</li>
<li>调整每个邻域节点（步骤4中找到的节点）权重以使它们更像输入向量。节点越靠近BMU，其权重调整的就越大。</li>
<li>N次迭代重复步骤2</li>
</ol>
<h1 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4 .代码实现"></a>4 .代码实现</h1><h2 id="4-1-数据"><a href="#4-1-数据" class="headerlink" title="4.1 数据"></a>4.1 数据</h2><p>数据是地铁隧道监控数据的330测区一段时间的收集的数据。具体数据如下图。</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/som%E7%AE%97%E6%B3%95/%E6%9D%A5%E8%BD%A6%E6%95%B0%E6%8D%AE.png" alt=""></p>
<p>将数据切分为2000份，其中包含来车的波形数据有148份。</p>
<p>4.2 数据特征提取。</p>
<p>这里数据特征提取采用自编码网络。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras.models import Model</span><br><span class="line">from keras.layers import Dense, Input</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from keras.optimizers import Adam</span><br><span class="line"></span><br><span class="line">data_in=data</span><br><span class="line">data_label=label</span><br><span class="line"># 建立模型</span><br><span class="line"># in order to plot in a 2D figure</span><br><span class="line">encoding_dim = 2  # encoding_dim，要压缩成的维度。</span><br><span class="line"></span><br><span class="line"># this is our input placeholder</span><br><span class="line">input_img = Input(shape=(1500,))</span><br><span class="line"></span><br><span class="line"># encoder layers</span><br><span class="line">encoded = Dense(512, activation=&apos;relu&apos;)(input_img)</span><br><span class="line">encoded = Dense(128, activation=&apos;relu&apos;)(encoded)</span><br><span class="line">encoded = Dense(32, activation=&apos;relu&apos;)(encoded)</span><br><span class="line">encoder_output = Dense(encoding_dim)(encoded)</span><br><span class="line"></span><br><span class="line"># decoder layers</span><br><span class="line">decoded = Dense(32, activation=&apos;relu&apos;)(encoder_output)</span><br><span class="line">decoded = Dense(128, activation=&apos;relu&apos;)(decoded)</span><br><span class="line">decoded = Dense(512, activation=&apos;relu&apos;)(decoded)</span><br><span class="line">decoded = Dense(1500, activation=&apos;tanh&apos;)(decoded)</span><br><span class="line"></span><br><span class="line"># 接下来直接用 Model 这个模块来组建模型，输入就是2000份波形，输出是解压的最后的结果。</span><br><span class="line"># construct the autoencoder model</span><br><span class="line">autoencoder = Model(input=input_img, output=decoded)</span><br><span class="line"># 当我们想要看由 1500 压缩到 2维后，这个结果是什么样的时候，也可以只单独组建压缩的板块，</span><br><span class="line"># 此时它的输入是[2000,1500]的数组，输出是压缩环节的最后结果。</span><br><span class="line"># construct the encoder model for plotting</span><br><span class="line">encoder = Model(input=input_img, output=encoder_output)</span><br><span class="line"></span><br><span class="line"># 激活模型</span><br><span class="line"># 接下来是编译自编码这个模型，优化器用的是 adam，损失函数用的是 mse。</span><br><span class="line"># compile autoencoder</span><br><span class="line">adam = Adam(lr=1e-4)</span><br><span class="line">autoencoder.compile(optimizer=&apos;adam&apos;, loss=&apos;mse&apos;)</span><br><span class="line"># 训练模型</span><br><span class="line"># training</span><br><span class="line">autoencoder.fit(data_in, data_in,</span><br><span class="line">                nb_epoch=20,</span><br><span class="line">                batch_size=256,</span><br><span class="line">                shuffle=True)</span><br><span class="line"># 可视化结果</span><br><span class="line"># 最后看到可视化的结果，自编码模型可以把这俩类波形给区分开来，</span><br><span class="line"># 我们可以用自编码这个过程来作为一个特征压缩的方法，</span><br><span class="line"># 和PCA的功能一样，效果要比它好一些，因为它是非线性的结构</span><br><span class="line"># plotting</span><br><span class="line">encoded_imgs = encoder.predict(data_in)</span><br><span class="line"># io.savemat(&apos;data2000.mat&apos;,&#123;&apos;data&apos;:encoded_imgs&#125;)</span><br><span class="line">plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=label)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>自编码提取的特征如下图：</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/som%E7%AE%97%E6%B3%95/%E8%87%AA%E7%BC%96%E7%A0%81%E7%89%B9%E5%BE%81%E5%8E%8B%E7%BC%A9.jpg" alt=""></p>
<p>4.3 SOM聚类</p>
<p>采用SOM模型对数据聚类。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line"># np.random.seed(1337)</span><br><span class="line"></span><br><span class="line">class SOM(object):</span><br><span class="line">    def __init__(self, X, output, iteration, batch_size):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param X:  形状是N*D， 输入样本有N个,每个D维</span><br><span class="line">        :param output: (n,m)一个元组，为输出层的形状是一个n*m的二维矩阵</span><br><span class="line">        :param iteration:迭代次数</span><br><span class="line">        :param batch_size:每次迭代时的样本数量</span><br><span class="line">        初始化一个权值矩阵，形状为D*(n*m)，即有n*m权值向量，每个D维</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.X = X</span><br><span class="line">        self.output = output</span><br><span class="line">        self.iteration = iteration</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.W = np.random.rand(X.shape[1], output[0] * output[1])</span><br><span class="line">        # print (self.W.shape)</span><br><span class="line"></span><br><span class="line">    def GetN(self, t):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param t:时间t, 这里用迭代次数来表示时间</span><br><span class="line">        :return: 返回一个整数，表示拓扑距离，时间越大，拓扑邻域越小</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        a = min(self.output)</span><br><span class="line">        return int(a-float(a)*t/self.iteration)</span><br><span class="line"></span><br><span class="line">    def Geteta(self, t, n):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param t: 时间t, 这里用迭代次数来表示时间</span><br><span class="line">        :param n: 拓扑距离</span><br><span class="line">        :return: 返回学习率，</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return np.power(np.e, -n)/(t+2)</span><br><span class="line"></span><br><span class="line">    def updata_W(self, X, t, winner):</span><br><span class="line">        N = self.GetN(t)</span><br><span class="line">        for x, i in enumerate(winner):</span><br><span class="line">            to_update = self.getneighbor(i, N)</span><br><span class="line">            for j in range(N+1):</span><br><span class="line">                e = self.Geteta(t, j)</span><br><span class="line">                for w in to_update[j]:</span><br><span class="line">                    self.W[:, w] = np.add(self.W[:,w], e*(X[x,:] - self.W[:,w]))</span><br><span class="line"></span><br><span class="line">    def getneighbor(self, index, N):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param index:获胜神经元的下标</span><br><span class="line">        :param N: 邻域半径</span><br><span class="line">        :return ans: 返回一个集合列表，分别是不同邻域半径内需要更新的神经元坐标</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        a, b = self.output</span><br><span class="line">        length = a*b</span><br><span class="line">        def distence(index1, index2):</span><br><span class="line">            i1_a, i1_b = index1 // a, index1 % b</span><br><span class="line">            i2_a, i2_b = index2 // a, index2 % b</span><br><span class="line">            return np.abs(i1_a - i2_a), np.abs(i1_b - i2_b)</span><br><span class="line"></span><br><span class="line">        ans = [set() for i in range(N+1)]</span><br><span class="line">        for i in range(length):</span><br><span class="line">            dist_a, dist_b = distence(i, index)</span><br><span class="line">            if dist_a &lt;= N and dist_b &lt;= N: ans[max(dist_a, dist_b)].add(i)</span><br><span class="line">        return ans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def train(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        train_Y:训练样本与形状为batch_size*(n*m)</span><br><span class="line">        winner:一个一维向量，batch_size个获胜神经元的下标</span><br><span class="line">        :return:返回值是调整后的W</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        count = 0</span><br><span class="line">        while self.iteration &gt; count:</span><br><span class="line">            train_X = self.X[np.random.choice(self.X.shape[0], self.batch_size)]</span><br><span class="line">            normal_W(self.W)</span><br><span class="line">            normal_X(train_X)</span><br><span class="line">            train_Y = train_X.dot(self.W)</span><br><span class="line">            winner = np.argmax(train_Y, axis=1).tolist()</span><br><span class="line">            self.updata_W(train_X, count, winner)</span><br><span class="line">            count += 1</span><br><span class="line">            # print(winner)</span><br><span class="line">        return self.W</span><br><span class="line"></span><br><span class="line">    def train_result(self):</span><br><span class="line">        normal_X(self.X)</span><br><span class="line">        train_Y = self.X.dot(self.W)</span><br><span class="line">        # winner = np.argmax(train_Y, axis=1).tolist()</span><br><span class="line">        winner = np.argmax(train_Y, axis=1)</span><br><span class="line">        # print (winner)</span><br><span class="line">        return winner</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def normal_X(X):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    :param X:二维矩阵，N*D，N个D维的数据</span><br><span class="line">    :return: 将X归一化的结果</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    N, D = X.shape</span><br><span class="line">    for i in range(N):</span><br><span class="line">        temp = np.sum(np.multiply(X[i], X[i]))</span><br><span class="line">        X[i] /= np.sqrt(temp)</span><br><span class="line">    return X</span><br><span class="line">def normal_W(W):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    :param W:二维矩阵，D*(n*m)，D个n*m维的数据</span><br><span class="line">    :return: 将W归一化的结果</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for i in range(W.shape[1]):</span><br><span class="line">        temp = np.sum(np.multiply(W[:,i], W[:,i]))</span><br><span class="line">        W[:, i] /= np.sqrt(temp)</span><br><span class="line">    return W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_car=io.loadmat(&apos;data2000.mat&apos;)</span><br><span class="line">data_car_data=data_car[&apos;data&apos;]</span><br><span class="line">data_car_data=np.array(data_car_data)</span><br><span class="line"></span><br><span class="line">size=(10,10)</span><br><span class="line">som = SOM(data_car_data, size, 200, 30)</span><br><span class="line">som.train()</span><br><span class="line">res = som.train_result()</span><br><span class="line"># print(res)</span><br><span class="line">#绘图</span><br><span class="line">data_drow=[]</span><br><span class="line">for i in range(len(res)):</span><br><span class="line">    data_x_y=res[i]</span><br><span class="line">    data_drow.append([math.floor(data_x_y/size[1]),data_x_y%size[1]])</span><br><span class="line">print(data_drow)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">data_x=[]</span><br><span class="line">data_y=[]</span><br><span class="line">for i in range(len(data_drow)):</span><br><span class="line">    data_x.append(data_drow[i][0])</span><br><span class="line">    data_y.append(data_drow[i][1])</span><br><span class="line"># print(data_x)</span><br><span class="line"># print(data_y)</span><br><span class="line">plt.scatter(data_x, data_y,c=label)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>聚类效果如下图：</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/som%E7%AE%97%E6%B3%95/SOM%E8%81%9A%E7%B1%BB.jpg" alt=""></p>
<p>当然也可以直接将数据传入SOM模型聚类。效果如下图：</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/som%E7%AE%97%E6%B3%95/SOM%E7%9B%B4%E6%8E%A5%E8%81%9A%E7%B1%BB.png" alt=""></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>人脸识别</title>
    <url>/2019/11/25/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/</url>
    <content><![CDATA[<p> 人脸识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部识别的一系列相关技术，通常也叫做人像识别、面部识别。 </p>
<a id="more"></a>

<h1 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1.环境准备"></a>1.环境准备</h1><h2 id="1-1-安装-Anconda，创建新环境。"><a href="#1-1-安装-Anconda，创建新环境。" class="headerlink" title="1.1 安装 Anconda，创建新环境。"></a>1.1 安装 <a href="https://www.jianshu.com/p/742dc4d8f4c5" target="_blank" rel="noopener">Anconda</a>，创建新环境。</h2><h2 id="1-2-安装pycharm"><a href="#1-2-安装pycharm" class="headerlink" title="1.2 安装pycharm"></a>1.2 安装<a href="https://www.runoob.com/w3cnote/pycharm-windows-install.html" target="_blank" rel="noopener">pycharm</a></h2><h2 id="1-2-在新环境中安装-Opencv、os、sleep等第三方包。"><a href="#1-2-在新环境中安装-Opencv、os、sleep等第三方包。" class="headerlink" title="1.2 在新环境中安装 Opencv、os、sleep等第三方包。"></a>1.2 在新环境中安装 Opencv、os、sleep等第三方包。</h2><p><strong>激活新环境，pip 安装第三方包。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install cv2</span><br><span class="line">pip install os</span><br><span class="line">pip install sleep</span><br></pre></td></tr></table></figure>



<h1 id="2-人脸识别流程"><a href="#2-人脸识别流程" class="headerlink" title="2.人脸识别流程"></a>2.人脸识别流程</h1><p>人脸识别系统主要包括四个组成部分，分别为：人脸图像采集及检测、人脸图像预处理、人脸图像特征提取以及匹配与识别。</p>
<h2 id="2-1-人脸图像采集及检测"><a href="#2-1-人脸图像采集及检测" class="headerlink" title="2.1 人脸图像采集及检测"></a>2.1 人脸图像采集及检测</h2><p>人脸图像采集：不同的人脸图像都能通过摄像镜头采集下来，比如静态图像、动态图像、不同的位置、不同表情等方面都可以得到很好的采集。当用户在采集设备的拍摄范围内时，采集设备会自动搜索并拍摄用户的人脸图像。</p>
<p>人脸检测：人脸检测在实际中主要用于人脸识别的预处理，即在图像中准确标定出人脸的位置和大小。人脸图像中包含的模式特征十分丰富，如直方图特征、颜色特征、模板特征、结构特征及Haar特征等。人脸检测就是把这其中有用的信息挑出来，并利用这些特征实现人脸检测。</p>
<p>主流的人脸检测方法基于以上特征采用Adaboost学习算法，Adaboost算法是一种用来分类的方法，它把一些比较弱的分类方法合在一起，组合出新的很强的分类方法。</p>
<p>人脸检测过程中使用Adaboost算法挑选出一些最能代表人脸的矩形特征(弱分类器)，按照加权投票的方式将弱分类器构造为一个强分类器，再将训练得到的若干强分类器串联组成一个级联结构的层叠分类器，有效地提高分类器的检测速度。</p>
<h2 id="2-2-人脸图像预处理"><a href="#2-2-人脸图像预处理" class="headerlink" title="2.2 人脸图像预处理"></a>2.2 人脸图像预处理</h2><p>人脸图像预处理：对于人脸的图像预处理是基于人脸检测结果，对图像进行处理并最终服务于特征提取的过程。系统获取的原始图像由于受到各种条件的限制和随机干扰，往往不能直接使用，必须在图像处理的早期阶段对它进行灰度校正、噪声过滤等图像预处理。对于人脸图像而言，其预处理过程主要包括人脸图像的光线补偿、灰度变换、直方图均衡化、归一化、几何校正、滤波以及锐化等。</p>
<h2 id="2-3-人脸图像特征提取"><a href="#2-3-人脸图像特征提取" class="headerlink" title="2.3 人脸图像特征提取"></a>2.3 人脸图像特征提取</h2><p>人脸图像特征提取：人脸识别系统可使用的特征通常分为视觉特征、像素统计特征、人脸图像变换系数特征、人脸图像代数特征等。人脸特征提取就是针对人脸的某些特征进行的。人脸特征提取，也称人脸表征，它是对人脸进行特征建模的过程。</p>
<h2 id="2-4-人脸图像匹配与识别"><a href="#2-4-人脸图像匹配与识别" class="headerlink" title="2.4 人脸图像匹配与识别"></a>2.4 人脸图像匹配与识别</h2><p>人脸图像匹配与识别：提取的人脸图像的特征数据与数据库中存储的特征模板进行搜索匹配，通过设定一个阈值，当相似度超过这一阈值，则把匹配得到的结果输出。人脸识别就是将待识别的人脸特征与已得到的人脸特征模板进行比较，根据相似程度对人脸的身份信息进行判断。这一过程又分为两类：一类是确认，是一对一进行图像比较的过程，另一类是辨认，是一对多进行图像匹配对比的过程。</p>
<h1 id="3-人脸识别程序实现"><a href="#3-人脸识别程序实现" class="headerlink" title="3 人脸识别程序实现"></a>3 人脸识别程序实现</h1><h2 id="3-1-人脸图像采集及检测"><a href="#3-1-人脸图像采集及检测" class="headerlink" title="3.1 人脸图像采集及检测"></a>3.1 人脸图像采集及检测</h2><p><strong>人脸保存</strong>。新建目录 newface，用来保存人脸，代码奉上。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def find_dir(Person_name):</span><br><span class="line">    # 读取目录</span><br><span class="line">    findfile = os.listdir(&quot;newface&quot;)</span><br><span class="line">    # 转列表 然后从新排序</span><br><span class="line">    list = []</span><br><span class="line">    for i in findfile:</span><br><span class="line">        list.append(int(i.split(&apos;_&apos;)[0]))</span><br><span class="line">    list.sort()  # 大小排序</span><br><span class="line">    # 判断目录是否为空 ，空写入0 否则按照顺序添加</span><br><span class="line">    if list == []:</span><br><span class="line">        os.mkdir(&apos;newface/0_%s&apos; % Person_name)</span><br><span class="line">        folder_number = &apos;0&apos;</span><br><span class="line">    else:</span><br><span class="line">        # 目录不为空，直接添加</span><br><span class="line">        folder_number = str(int(list[-1]) + 1)</span><br><span class="line">        os.mkdir(&apos;newface/%s_%s&apos; % (folder_number, Person_name))</span><br><span class="line">    return folder_number</span><br></pre></td></tr></table></figure>

<p>find_dir函数：在newface文件夹下生成以人名命名的文件夹，文件下保存该人的人脸</p>
<p><strong>人脸采集。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 收集人脸</span><br><span class="line">def collect_face():</span><br><span class="line">    # 导入识别人脸模块</span><br><span class="line">    faceModel= cv2.CascadeClassifier(&apos;face.xml&apos;)</span><br><span class="line">    # 打开摄像头收集人脸</span><br><span class="line">    # VideoCapture  0表示本地摄像头，1表示外部摄像头</span><br><span class="line">    capture = cv2.VideoCapture(0)</span><br><span class="line">    # 输入识别人的姓名</span><br><span class="line">    PersonName = input(&apos;请输入姓名:--&gt;&apos;)</span><br><span class="line">    FolderName = find_dir(PersonName)</span><br><span class="line">    # 定义变量，后面要用</span><br><span class="line">    counter = 0</span><br><span class="line">    Number_samples = 20</span><br><span class="line">    sleep(2)</span><br><span class="line">    while True:</span><br><span class="line">        # 读取摄像头数据，ret=ture or false 表示是否有人脸，frame是一帧一帧的图片</span><br><span class="line">        ret, frame = capture.read()</span><br><span class="line">        #灰度化图片，face.xml识别的是灰度图像中的人脸</span><br><span class="line">        gray = cv2.cvtColor(src=frame, code=cv2.COLOR_RGB2GRAY)</span><br><span class="line">        # 检测人脸 给我了一个坐标</span><br><span class="line">        faces = faceModel.detectMultiScale(frame, scaleFactor=1.2, minSize=(200, 200))</span><br><span class="line">        # 标记人脸&amp;取图片</span><br><span class="line">        for (x, y, w, h) in faces:</span><br><span class="line">            #记数，记录采集的图片的数目</span><br><span class="line">            counter += 1</span><br><span class="line">            #画摄像头人脸边框</span><br><span class="line">            cv2.rectangle(frame, pt1=(x, y), pt2=(x + w, y + h), color=(0, 255, 0), thickness=2)</span><br><span class="line">            #保存图片</span><br><span class="line">            counter=str(counter)</span><br><span class="line">            final_name = &apos;newface/&apos; + FolderName + &apos;_&apos; + PersonName + &apos;/&apos; + FolderName + &apos;_&apos; + &apos;%s.jpg&apos; % str(</span><br><span class="line">                PersonName + &apos;_&apos; + counter)</span><br><span class="line">            counter = int(counter)</span><br><span class="line">            SaveImg = Image.fromarray(gray)  # PIL直接读取 数组 然后存图片</span><br><span class="line">            SaveImg.save(fp=final_name)</span><br><span class="line">            Img = Image.open(fp=final_name)</span><br><span class="line">            # 截取头像 需要适当加大截取框</span><br><span class="line">            CropImg = Img.crop((x - 40, y - 40, x + w + 40, y + h + 40))  </span><br><span class="line">            CropImg.save(fp=final_name)</span><br><span class="line">            #显示采集的图片数目</span><br><span class="line">            cv2.putText(frame, &apos;%s Images collected&apos; % str(counter), org=(x, y - 10), fontScale=1,fontFace=cv2.FONT_HERSHEY_COMPLEX, color=(0, 255, 0), thickness=1)</span><br><span class="line">            #显示图片</span><br><span class="line">            cv2.imshow(&apos;LIVEface&apos;, frame)</span><br><span class="line">        # 如果已经收到足够量的样本就停</span><br><span class="line">        if counter &gt; Number_samples:</span><br><span class="line">            break</span><br><span class="line">        # 或者x退出</span><br><span class="line">        if cv2.waitKey(40) &amp; 0xFF == ord(&apos;x&apos;):</span><br><span class="line">            break</span><br><span class="line">    capture.release()</span><br><span class="line">    cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<p>代码中face.xml文件为cv2\data\haarcascade_frontalface_default.xml文件，我这里直接拿出来了。</p>
<p>代码中sheep（2）,打开机器的摄像头需要点时间，休眠等待下。</p>
<p>collect_face函数：用来收集人脸。采集人脸的方法有很多，基于图像处理和深度学习算法都可以用来采集人脸，但我做的效果都没有cv2自带的好，这里就直接用cv2中现有的算法采集人脸。</p>
<h2 id="3-2-人脸图像预处理"><a href="#3-2-人脸图像预处理" class="headerlink" title="3.2 人脸图像预处理"></a>3.2 人脸图像预处理</h2><p>将采集到的图像处理为相同大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_dir</span><span class="params">(address)</span>:</span></span><br><span class="line">    <span class="comment"># 读取目录</span></span><br><span class="line">    filepath = <span class="string">'D:\\pycharm projects\\OpenCV\\wj\\newface'</span></span><br><span class="line">    fileNames = os.listdir(filepath)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> fileNames:</span><br><span class="line">        newDir = filepath + <span class="string">'/'</span> + file</span><br><span class="line">        path = newDir</span><br><span class="line">        fileNames_1 = os.listdir(path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> fileNames_1:</span><br><span class="line">            img = cv2.imread(path+<span class="string">'/'</span>+i,<span class="number">0</span>)</span><br><span class="line">            pic = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>), interpolation=cv2.INTER_CUBIC)</span><br><span class="line">            final_name = <span class="string">'train_folder/'</span> +i</span><br><span class="line"></span><br><span class="line">            SaveImg = Image.fromarray(pic)  <span class="comment"># PIL直接读取 数组 然后存图片</span></span><br><span class="line">            SaveImg.save(fp=final_name)</span><br></pre></td></tr></table></figure>

<p>find_dir函数：将采集到的人脸图像处理为200*200大小。</p>
<p>3.3 人脸图像的特征提取与模型训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readImages</span><span class="params">()</span>:</span></span><br><span class="line">    x, y = [], []</span><br><span class="line">    filepath = <span class="string">r'D:\pycharm projects\OpenCV\wj\train_folder'</span></span><br><span class="line">    fileNames = os.listdir(filepath)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> fileNames:</span><br><span class="line">        newDir =filepath+<span class="string">"\\"</span>+file</span><br><span class="line">        img = cv2.imread(newDir, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">        img = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line">        x.append(np.asarray(img, dtype=np.uint8)) <span class="comment">#x是训练数据</span></span><br><span class="line">        y.append(int(file.split(<span class="string">'_'</span>)[<span class="number">0</span>]))         <span class="comment">#y是训练数据标签</span></span><br><span class="line"></span><br><span class="line">    y = np.asarray(y, dtype=np.int32)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型训练与保存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">face_rec</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    x, y = readImages()</span><br><span class="line">    <span class="comment"># print(x[0].shape)</span></span><br><span class="line">    <span class="comment"># 人脸识别的模型</span></span><br><span class="line">    model = cv2.face.EigenFaceRecognizer_create()</span><br><span class="line">    <span class="comment"># fisherfaces算法的模型</span></span><br><span class="line">    <span class="comment"># model = cv2.face.FisherFaceRecognizer_create()</span></span><br><span class="line">    <span class="comment"># LBPH算法的模型</span></span><br><span class="line">    <span class="comment"># model = cv2.face.LBPHFaceRecognizer_create()</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Eigenfaces和Fisherfaces 预测时候产生0到20000的评分</span></span><br><span class="line"><span class="string">        低于4000 5000 的评分都是相当可靠的</span></span><br><span class="line"><span class="string">    LBPH 产生评分不同，低于50是可靠的 高于80是不可靠的</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    model.train(np.asarray(x), np.asarray(y))</span><br><span class="line">    <span class="comment">#model.save("facemodel.txt")</span></span><br></pre></td></tr></table></figure>

<p>这里模型同样用的cv2现有的模型，感兴趣的可以尝试fisherfaces算法的模型和LBPH算法的模型。</p>
<p>3.4 人脸识别。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">orc</span><span class="params">(model)</span>:</span></span><br><span class="line">    faceModel = cv2.CascadeClassifier(<span class="string">'face.xml'</span>)</span><br><span class="line">    capture = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 读取摄像头内容</span></span><br><span class="line">        ret, frame = capture.read()</span><br><span class="line">        <span class="comment"># 3维数据变一维</span></span><br><span class="line">        gray = cv2.cvtColor(src=frame, code=cv2.COLOR_RGB2GRAY)</span><br><span class="line">        <span class="comment"># 检测人脸</span></span><br><span class="line">        faces = faceModel.detectMultiScale(gray, scaleFactor=<span class="number">1.2</span>)</span><br><span class="line">        <span class="comment"># 标记人脸</span></span><br><span class="line">        <span class="keyword">for</span> (x, y, w, h) <span class="keyword">in</span> faces:</span><br><span class="line">            cv2.rectangle(frame, pt1=(x, y), pt2=(x + w, y + h), color=(<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), thickness=<span class="number">2</span>)</span><br><span class="line">            SaveImg = Image.fromarray(gray) <span class="comment"># PIL直接读取 数组</span></span><br><span class="line">            SaveImg=np.array(SaveImg)</span><br><span class="line">            <span class="comment">#图片大小处理</span></span><br><span class="line">            pic = cv2.resize(SaveImg, (<span class="number">200</span>, <span class="number">200</span>), interpolation=cv2.INTER_CUBIC)</span><br><span class="line"></span><br><span class="line">            a=int(model.predict(pic)[<span class="number">0</span>])</span><br><span class="line">            laber=[<span class="string">'wangjie'</span>,<span class="string">'wangkailun'</span>,<span class="string">'fengjingbao'</span>]</span><br><span class="line">            cv2.putText(frame, laber[a], (x, y), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, <span class="number">1</span>, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">            cv2.imshow(<span class="string">'face'</span>, frame)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xff</span> == ord(<span class="string">'x'</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    capture.release()</span><br><span class="line">    cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<p>orc函数为人脸识别，传入的model为3.3中保存的模型，最后效果如下图。</p>
<p><img src="https://blog-1300775928.cos.ap-chengdu.myqcloud.com/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/image-20191125220743144.png" alt=""></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>java环境配置</title>
    <url>/2019/11/24/java%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="java环境配置"><a href="#java环境配置" class="headerlink" title="java环境配置"></a>java环境配置</h1><h2 id="1-jdk1-8下载"><a href="#1-jdk1-8下载" class="headerlink" title="1.jdk1.8下载"></a>1.jdk1.8下载</h2><a id="more"></a>

<p>下载地址：<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>
<p>1.1 打开链接如图，点击Downloads。</p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574519694569.png" alt="1574519694569"></p>
<p><strong>1.2 网页下翻，找到Java SE Development Kit 8u231,点击接受License Agreement后找到对应jdk版本下载</strong>.</p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574520000241.png" alt="1574520000241"></p>
<h2 id="2-jdk1-8安装"><a href="#2-jdk1-8安装" class="headerlink" title="2.jdk1.8安装"></a>2.jdk1.8安装</h2><p><strong>2.1 解压刚下载的文件后打开，会出现如下窗口，安装Java SE，直接点击下一步。</strong></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574520521565.png" alt="1574520521565"></p>
<p><strong>2.2 选择安装目录，后点击下一步。</strong></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574520735939.png" alt="1574520735939"></p>
<p><strong>2.3  java SE 安装完后回弹出窗口java安装</strong>。选择安装位置，点击下一步</p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574521068513.png" alt="1574521068513"></p>
<p><strong>2.4 等待安装完成即可。</strong></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574521148433.png" alt="1574521148433"></p>
<h2 id="3-环境配置"><a href="#3-环境配置" class="headerlink" title="3 环境配置"></a>3 环境配置</h2><p><strong>3.1 进入系统的环境变量，点击环境变量</strong>。</p>
<p>（鼠标<strong>右键</strong>我的电脑，点击<strong>属性</strong>后点击 <strong>高级系统设置</strong>）</p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574522143475.png" alt="1574522143475"></p>
<p><strong>3.2  配置变量一  ：JAVA_HOMR(jdk安装的所在位置）</strong></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574522346666.png" alt="1574522346666"></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574522498231.png" alt="1574522498231"></p>
<p><strong>3.3 配置变量二 PATH 。</strong></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574522620953.png" alt="1574522620953"></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574522995349.png" alt="1574522995349"></p>
<p><strong>3.4 配置变量三 CLASSPATH</strong></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574523141637.png" alt="1574523141637"></p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574523307793.png" alt="1574523307793"></p>
<p><strong>3.5 测试是否成功</strong></p>
<p>运行框中输入cmd ，后在dos窗口输入javac，出现下图则恭喜配置成功。</p>
<p><img src="C:%5CUsers%5Cwj123456%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574523438453.png" alt="1574523438453"></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/11/23/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
